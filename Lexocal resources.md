# Задание
Дана выборка из датасета WordSim353. Ваша задача, используя тезаурус WordNet, вычислить оценки близости для всех элементов соответствующих синсетов представленных пар слов. В качестве близости двух слов использовать наибольшую близость среди соответствующих элементов рассматриваемых синсетов. В качестве меры близости использовать: близость на основе пути `(path_similarity)`, меру Leacock-Chodorow `(lch_similarity)` и меру Wu-Palmer `(wup_similarity)`. Вычислить коэффициент ранговой корреляции Спирмена для каждой меры близости, используя известную экспертную оценку (колонка `Score` в выборке).

## Теория
<i>Оценка на основе пути</i> - указывает, насколько схожи смыслы двух слов на основе кратчайшего пути, соединяющего смыслы в отношении гипероним / гипоним \
\
<i>Оценка на основе меры Leacock - Chodorow</i> - указывает, насколько схожи смыслы двух слов на основе кратчайшего пути как в методе на основе пути, но ещё учитывает максимальную глубину, на которой встречаются смыслы\
\
<i>Оценка на основе меры Wu-Palmer </i> - указывает, насколько похожи смыслы слов на основе глубины смыслов слов и глубины их общего узла \
\
<i>Коэффициент ранговой корреляции Спирмена</i> – это количественная оценка
статистического изучения связи между явлениями, используемая в непараметрических методах Непараметрические методы позволяют обрабатывать данные
"низкого качества" из выборок малого объёма с переменными, про распределение которых мало что или вообще ничего неизвестно.Коэффициент ранговой корреляции Спирмена относится к показателям
оценки тесноты связи.

### Импортируем библиотеки
```python 
import nltk  # основной тулбокс
from nltk.corpus import wordnet as wn  # импортируем ворднет
from itertools import product  # считает декартово произведение входных последовательностей, аналог вложенным циклам for ... in в выражении генератора
from scipy.stats import spearmanr  # 

# nltk.download('wordnet')  # достаточно 1 раз загрузить пакет, затем можно закомментить
```

### Готовим данные к работе
Затем импортируем данные из подготовленного текстового файла. Файл содержит набор пар слов (только имён существительных), для которых известны экспертные оценки сходства.

Строим ассоциативный массив "пара слов -- оценка близости".
```python
with open("sample4.csv", encoding="utf-8") as file:  # with open - открывает и самостоятельно затем закрывает файл, в отличие от просто open()
    triples = [line.strip().split(',') for line in file.readlines()]  # считываем строку, создаём вложенный список из 2х слов и их экспертной оценки
    score_map = {tuple(triple[:2]): float(triple[2]) for triple in triples}  # создаём словарь, где ключ - пара слов, а значение - оценка
```
Отметим, что из исходного набора данных мы взяли только экспертные оценки сходства (similarity) и только для существительных.


### Примеры оценок
У слов может быть по несколько значений, которые различаются в WordNet. Здесь -- ради примера -- мы будем "жадно" выбирать первое попавшееся, но далее будем работать с ними иначе.
```python
for w1, w2 in list(score_map)[:2]:  # для примера всего 2 элемента сделаем
    print("\nWords: %s-%s\nGround truth score: %.2f" % (w1, w2, score_map[(w1, w2)]))  # экспертная оценка

    ss1 = wn.synset(w1 + ".n.01")  # для каждого слова берём только первое значение (.01) и только для существительных (.n), создаём синсет
    ss2 = wn.synset(w2 + ".n.01")

    #'.... %.3f ' % работает по принципу: на месте % встанет то, что идёт после последнего %, формат задаётся .3f (.2f и др)

    print("\nPath: %.3f" % ss1.path_similarity(ss2), end=' ')  # ищем сходство на основе пути
    print("\nwup: %.3f" % ss1.wup_similarity(ss2), end=' ')  # на основе меры Wu-Palmer
    print("\nshortest_path: %.3f" % ss1.shortest_path_distance(ss2))
    
```


### Вычисляем для всех пар несколько оценок
```python
list_pairs = list(score_map)
wup_list, true_list, path_list, lch_list = [], [], [], []  # подготавливаем списки для последующей записи

for w1, w2 in list_pairs:

    all_w1 = wn.synsets(w1, pos='n')  # Только существительные (noun)
    all_w2 = wn.synsets(w2, pos="n")
    # считаем интересующие нас оценки, вычисляя декартово расстояние с помощью product() и добавляя в соответсвующий список
    wup = max([it1.wup_similarity(it2) for it1, it2 in product(all_w1, all_w2)])
    wup_list.append(wup)

    path = max([it1.path_similarity(it2) for it1, it2 in product(all_w1, all_w2)])
    path_list.append(path)

    lch = max([it1.lch_similarity(it2) for it1, it2 in product(all_w1, all_w2)])
    lch_list.append(lch)

    true_list.append(score_map[(w1, w2)])

```

### Вычисляем ранговую корреляцию Спирмена для каждого метода
```python
coef, p = spearmanr(wup_list, true_list)
print("wup Spearman R: %.4f" % coef)

coef, p1 = spearmanr(path_list, true_list)
print("path Spearman R: %.4f" % coef)

coef1, p2 = spearmanr(lch_list, true_list)
print("lch Spearman R: %.4f" % coef1)
```

### При помощи метода hyponyms() найдите количество гипонимов для синсета wood.n.01
```python
s1 = wn.synset("wood" + ".n.01")
print(f'Количество гипонимов к слову wood.n.01: {len(s1.hyponyms())},\nЗначение первого гипонима: {s1.hyponyms()[0]}')
```